"""
ITD105 - Big Data Analytics
Lab Exercise #2 - Part 1: Classification Task
PIMA Indians Diabetes Dataset - Binary Classification
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import KFold, LeaveOneOut, cross_val_predict
from sklearn.metrics import (
    accuracy_score, 
    log_loss, 
    roc_auc_score, 
    confusion_matrix, 
    classification_report
)
import joblib
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# 1. LOAD DATASET
# =============================================================================
print("="*80)
print("CLASSIFICATION TASK - PIMA INDIANS DIABETES DATASET")
print("="*80)

# Load the diabetes dataset
df = pd.read_csv('diabetes.csv')
print(f"\nüìä Dataset loaded successfully!")
print(f"   Shape: {df.shape}")
print(f"   Features: {df.columns.tolist()}")

# =============================================================================
# 2. DEFINE X (FEATURES) AND y (TARGET)
# =============================================================================
X = df.drop('Outcome', axis=1)
y = df['Outcome']

print(f"\n‚úÖ Features (X): {X.shape}")
print(f"‚úÖ Target (y): {y.shape}")
print(f"   Class distribution: {y.value_counts().to_dict()}")

# =============================================================================
# 3. CREATE PIPELINE (STANDARDSCALER + LOGISTIC REGRESSION)
# =============================================================================
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

print(f"\nüîß Pipeline created: StandardScaler ‚Üí LogisticRegression")

# =============================================================================
# 4. MODEL A - K-FOLD CROSS-VALIDATION (n_splits=10)
# =============================================================================
print("\n" + "="*80)
print("MODEL A: K-FOLD CROSS-VALIDATION (10 splits)")
print("="*80)

kfold = KFold(n_splits=10, shuffle=True, random_state=42)
y_pred_kfold = cross_val_predict(pipeline, X, y, cv=kfold, method='predict')
y_pred_proba_kfold = cross_val_predict(pipeline, X, y, cv=kfold, method='predict_proba')

# Calculate metrics for Model A
accuracy_kfold = accuracy_score(y, y_pred_kfold)
logloss_kfold = log_loss(y, y_pred_proba_kfold)
roc_auc_kfold = roc_auc_score(y, y_pred_proba_kfold[:, 1])
cm_kfold = confusion_matrix(y, y_pred_kfold)
cr_kfold = classification_report(y, y_pred_kfold)

print(f"\nüìà Performance Metrics (Model A):")
print(f"   Classification Accuracy: {accuracy_kfold:.4f}")
print(f"   Logarithmic Loss: {logloss_kfold:.4f}")
print(f"   Area Under ROC Curve: {roc_auc_kfold:.4f}")

print(f"\nüî¢ Confusion Matrix:")
print(f"   {cm_kfold}")

print(f"\nüìã Classification Report:")
print(cr_kfold)

# =============================================================================
# 5. MODEL B - LEAVE-ONE-OUT CROSS-VALIDATION (LOOCV)
# =============================================================================
print("\n" + "="*80)
print("MODEL B: LEAVE-ONE-OUT CROSS-VALIDATION (LOOCV)")
print("="*80)

loo = LeaveOneOut()
print(f"‚ö†Ô∏è  Note: LOOCV will train {X.shape[0]} models (one per sample)...")
print("   This may take a few minutes...")

y_pred_loo = cross_val_predict(pipeline, X, y, cv=loo, method='predict')
y_pred_proba_loo = cross_val_predict(pipeline, X, y, cv=loo, method='predict_proba')

# Calculate metrics for Model B
accuracy_loo = accuracy_score(y, y_pred_loo)
logloss_loo = log_loss(y, y_pred_proba_loo)
roc_auc_loo = roc_auc_score(y, y_pred_proba_loo[:, 1])
cm_loo = confusion_matrix(y, y_pred_loo)
cr_loo = classification_report(y, y_pred_loo)

print(f"\nüìà Performance Metrics (Model B):")
print(f"   Classification Accuracy: {accuracy_loo:.4f}")
print(f"   Logarithmic Loss: {logloss_loo:.4f}")
print(f"   Area Under ROC Curve: {roc_auc_loo:.4f}")

print(f"\nüî¢ Confusion Matrix:")
print(f"   {cm_loo}")

print(f"\nüìã Classification Report:")
print(cr_loo)

# =============================================================================
# 6. COMPARISON AND JUSTIFICATION
# =============================================================================
print("\n" + "="*80)
print("MODEL COMPARISON & SELECTION")
print("="*80)

print(f"\nüìä Side-by-Side Comparison:")
print(f"{'Metric':<30} {'Model A (K-Fold)':<20} {'Model B (LOOCV)':<20}")
print("-" * 70)
print(f"{'Accuracy':<30} {accuracy_kfold:<20.4f} {accuracy_loo:<20.4f}")
print(f"{'Logarithmic Loss':<30} {logloss_kfold:<20.4f} {logloss_loo:<20.4f}")
print(f"{'ROC-AUC Score':<30} {roc_auc_kfold:<20.4f} {roc_auc_loo:<20.4f}")

print("\n" + "="*80)
print("üèÜ SELECTED MODEL: MODEL A (K-FOLD CROSS-VALIDATION)")
print("="*80)

print("""
üìù JUSTIFICATION:

Model A (K-Fold Cross-Validation with 10 splits) is selected as the best approach 
for the following reasons:

1. ‚ö° COMPUTATIONAL EFFICIENCY:
   - K-Fold trains only 10 models vs. LOOCV's 768 models
   - Significantly faster training and evaluation time
   - More practical for deployment and iterative development

2. üìä COMPARABLE PERFORMANCE:
   - Accuracy, Log Loss, and ROC-AUC scores are very similar between both models
   - The minimal performance difference does not justify the computational cost of LOOCV

3. üéØ BALANCED BIAS-VARIANCE TRADEOFF:
   - K-Fold (k=10) provides a good balance between bias and variance
   - LOOCV has lower bias but higher variance in performance estimates
   - K-Fold offers more stable and generalizable results

4. üîÑ BETTER GENERALIZATION:
   - K-Fold provides multiple independent test sets (10 folds)
   - Reduces the risk of overfitting to a specific validation strategy
   - More representative of real-world model performance

5. üíº INDUSTRY STANDARD:
   - K-Fold (especially 10-fold) is widely adopted in machine learning practice
   - Easier to reproduce and compare with other studies
   - Better documentation and community support

CONCLUSION: Model A (K-Fold) is the optimal choice, offering excellent performance
with practical computational efficiency and robust validation methodology.
""")

# =============================================================================
# 7. TRAIN FINAL MODEL ON ENTIRE DATASET & SAVE
# =============================================================================
print("\n" + "="*80)
print("TRAINING FINAL MODEL ON ENTIRE DATASET")
print("="*80)

# Train the pipeline on the entire dataset
pipeline.fit(X, y)
print(f"\n‚úÖ Final model trained on all {X.shape[0]} samples")

# Save the model
model_filename = 'classification_model.joblib'
joblib.dump(pipeline, model_filename)
print(f"‚úÖ Model saved as: {model_filename}")

print("\n" + "="*80)
print("‚ú® CLASSIFICATION TASK COMPLETED SUCCESSFULLY!")
print("="*80)
print(f"\nüì¶ Next Steps:")
print(f"   1. Use '{model_filename}' in your Streamlit application")
print(f"   2. Load with: model = joblib.load('{model_filename}')")
print(f"   3. Make predictions with: model.predict(new_data)")
print("="*80)

